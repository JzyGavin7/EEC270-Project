{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP\n",
    "\n",
    "EEC 270 Website Fingerprinting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('Set2')\n",
    "\n",
    "data = pd.read_csv('data_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# seq_len, batch, feature_len\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(25, 200),\n",
    "            nn.BatchNorm1d(200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 31),\n",
    "            nn.BatchNorm1d(31),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.linear(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import random\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "\n",
    "X, y = data.values[:,:-1], data.values[:,-1]\n",
    "X, y = torch.Tensor(X), torch.Tensor(y)\n",
    "trans_X = X.reshape((len(X), 25))\n",
    "trans_X_ = X.reshape((5, len(X), 5))\n",
    "X = trans_X\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Split dataset into train and valid, with a ratio of 4:1\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "random.shuffle(indices)\n",
    "split = int(np.floor(0.2 * dataset_size))\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=train_sampler\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=valid_sampler\n",
    ")\n",
    "\n",
    "X_test, y_test = None, None\n",
    "for _, test_sample in enumerate(valid_loader):\n",
    "    X_test, y_test = test_sample[0], test_sample[1].long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training Loss: 0.6521893739700317 Accuracy: 0.8397328881469115\n",
      "Epoch 100 Training Loss: 0.29776132106781006 Accuracy: 0.8480801335559266\n",
      "Epoch 200 Training Loss: 0.22466903924942017 Accuracy: 0.8681135225375626\n",
      "Epoch 300 Training Loss: 0.26048704981803894 Accuracy: 0.8731218697829716\n",
      "Epoch 400 Training Loss: 0.27816393971443176 Accuracy: 0.8664440734557596\n",
      "Epoch 500 Training Loss: 0.26442909240722656 Accuracy: 0.8681135225375626\n",
      "Epoch 600 Training Loss: 0.23918648064136505 Accuracy: 0.8614357262103506\n",
      "Epoch 700 Training Loss: 0.22243428230285645 Accuracy: 0.8547579298831386\n",
      "Epoch 800 Training Loss: 0.23108530044555664 Accuracy: 0.8664440734557596\n",
      "Epoch 900 Training Loss: 0.2416336089372635 Accuracy: 0.8647746243739566\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('./mlp')\n",
    "\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for _, sample in enumerate(train_loader, 0):\n",
    "        model.zero_grad()\n",
    "        inputs, labels = sample\n",
    "        labels = labels.long()\n",
    "        # pred = nn.functional.softmax(model(inputs))\n",
    "        pred = model(inputs)\n",
    "        \n",
    "        # Training loss\n",
    "        loss = criterion(pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for sample in valid_loader:\n",
    "            inputs, labels = sample\n",
    "            labels = labels.long()\n",
    "            # pred = nn.functional.softmax(model(inputs))\n",
    "            pred = model(inputs)\n",
    "            # print(pred)\n",
    "            \n",
    "            _, predicted = torch.max(pred.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    writer.add_scalar('Loss/train', loss.item(), epoch)\n",
    "    writer.add_scalar('Accuracy/test', correct/total, epoch)\n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch', epoch, 'Training Loss:', loss.item(), 'Accuracy:', correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(X_test)\n",
    "accuracy_score(pred.argmax(axis=1), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mlp.pkl']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(model, 'mlp.pkl')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ef9226f5f281bed155f6877abe961f8893e93036b236b1a7e5395de73a55139a"
  },
  "kernelspec": {
   "display_name": "Python 3.6.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
